{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Ali0237/HF-Projects/blob/main/Intermediate/Conversational%20AI%20App/Chatbot_With_Memory_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e6fb93c5",
      "metadata": {
        "id": "e6fb93c5"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "760d8f11",
      "metadata": {
        "id": "760d8f11"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/GODEL-v1_1-large-seq2seq\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/GODEL-v1_1-large-seq2seq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "499e35cf",
      "metadata": {
        "id": "499e35cf"
      },
      "outputs": [],
      "source": [
        "# t2t_pipeline = pipeline(\n",
        "#     task = \"text2text-generation\",\n",
        "#     model = model,\n",
        "#     tokenizer = tokenizer\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "sBKiZUbUjIqg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBKiZUbUjIqg",
        "outputId": "0cf5ecfc-592a-4056-b9dc-20561b6d0cb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32102, 1024)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32102, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32102, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32102, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6bb99a69",
      "metadata": {
        "id": "6bb99a69"
      },
      "outputs": [],
      "source": [
        "# store conversation history\n",
        "history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff896c6",
      "metadata": {
        "id": "0ff896c6"
      },
      "outputs": [],
      "source": [
        "def generate_response(user_prompt):\n",
        "    # The user's input or question\n",
        "    instruction = user_prompt\n",
        "\n",
        "    # Combine the conversation history into a single context string\n",
        "    # Each entry in history is formatted as \"User: <user_input> Assistant: <assistant_response>\"\n",
        "    context = \" \".join([f\"User: {u} Assistant: {a}\" for u, a in history])\n",
        "\n",
        "    # Create the prompt by combining the user's instruction and the conversation context\n",
        "    # The [SEP] token separates the instruction from the context\n",
        "    prompt = f\"{instruction} [SEP] {context}\"\n",
        "\n",
        "    # Tokenize the prompt into input tensors for the model\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors='pt',  # Return PyTorch tensors\n",
        "        max_length=1024,      # Truncate if the prompt exceeds the max length\n",
        "        truncation=True       # Enable truncation\n",
        "    )\n",
        "\n",
        "    # Move the input tensors to the same device as the model (CPU or GPU)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate a response using the model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,       # Limit the length of the generated response\n",
        "        do_sample=True,       # Enable sampling for more diverse responses\n",
        "        top_p=0.9,            # Use nucleus sampling with a top-p value of 0.9\n",
        "        temperature=0.7       # Control the randomness of the response\n",
        "    )\n",
        "\n",
        "    # Decode the generated response from token IDs to a string\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True  # Remove special tokens from the output\n",
        "    )\n",
        "\n",
        "    # Append the user's input and the assistant's response to the conversation history\n",
        "    history.append((user_prompt, response))\n",
        "\n",
        "    # Return the generated response\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ede95a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ede95a9",
        "outputId": "100f6c2b-08a0-4420-b96b-906dd5a243f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GODEL Chatbot: Type 'exit' to quit.\n",
            "\n",
            "You: heyyy, whats up?\n",
            "Bot: Hi, just got back from the gym.\n",
            "\n",
            "You: noice man, getting them gains\n",
            "Bot: I'm just getting back from the gym.\n",
            "\n",
            "You: I know. what did you do in the gym?\n",
            "Bot: I just got back from the gym, and I'm still getting those gains.\n",
            "\n",
            "You: bro. just shut up\n",
            "Bot: The 'getting them gains' part is funny because I was a bit of a troll.\n",
            "\n",
            "You: lol\n",
            "Bot: I'm not a troll, but I do find it funny that you got back from the gym.\n",
            "\n",
            "You: exit\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Inform the user about the chatbot and how to exit\n",
        "    print(\"GODEL Chatbot: Type 'exit' to quit.\\n\")\n",
        "    \n",
        "    # Start an infinite loop to keep the conversation going\n",
        "    while True:\n",
        "        # Prompt the user for input\n",
        "        user_input = input(\"You: \")\n",
        "        \n",
        "        # Check if the user wants to exit the chat\n",
        "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "            break\n",
        "        \n",
        "        # Generate a response from the chatbot based on the user's input\n",
        "        reply = generate_response(user_input)\n",
        "        \n",
        "        # Display the chatbot's response\n",
        "        print(f\"Bot: {reply}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EsjN9Oh5k_fu",
      "metadata": {
        "id": "EsjN9Oh5k_fu"
      },
      "source": [
        "# Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d7tHdWVklDXt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7tHdWVklDXt",
        "outputId": "614ad390-572e-4cd8-9cfa-fbd0dbc19a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iDBMS9dHiokQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "iDBMS9dHiokQ",
        "outputId": "4a190114-8842-4a4c-dd39-3a023b885f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://57bf5bacb7f5c430e2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://57bf5bacb7f5c430e2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Global conversation history to store the chat between the user and the assistant\n",
        "ui_history = []\n",
        "\n",
        "# Function to handle user input and generate a response\n",
        "def chatbot(user_prompt):\n",
        "    # Combine the conversation history into a single context string\n",
        "    context = \" \".join([f\"User: {u} Assistant: {a}\" for u, a in ui_history])\n",
        "\n",
        "    # Create the prompt by combining the user's input and the conversation context\n",
        "    prompt = f\"{user_prompt} [SEP] {context}\"\n",
        "\n",
        "    # Tokenize the prompt into input tensors for the model\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "        truncation=True,      # Truncate if the prompt exceeds the max length\n",
        "        max_length=1024       # Limit the input length to 1024 tokens\n",
        "    )\n",
        "\n",
        "    # Move the input tensors to the same device as the model (CPU or GPU)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate a response using the model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,       # Limit the length of the generated response\n",
        "        do_sample=True,       # Enable sampling for more diverse responses\n",
        "        top_p=0.9,            # Use nucleus sampling with a top-p value of 0.9\n",
        "        temperature=0.7       # Control the randomness of the response\n",
        "    )\n",
        "\n",
        "    # Decode the generated response from token IDs to a string\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True  # Remove special tokens from the output\n",
        "    )\n",
        "\n",
        "    # Append the user's input and the assistant's response to the conversation history\n",
        "    ui_history.append((user_prompt, response))\n",
        "\n",
        "    # Format the conversation history into a list of messages for the Gradio chatbot UI\n",
        "    messages = []\n",
        "    for u, a in ui_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": u})       # Add user messages\n",
        "        messages.append({\"role\": \"assistant\", \"content\": a}) # Add assistant responses\n",
        "\n",
        "    # Return the formatted messages to update the chatbot UI\n",
        "    return messages\n",
        "\n",
        "# Function to clear the conversation history\n",
        "def clear_history():\n",
        "    ui_history.clear()  # Clear the global conversation history\n",
        "    return []           # Return an empty list to reset the chatbot UI\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    # Add a title to the chatbot interface\n",
        "    gr.Markdown(\"# ðŸ¤– GODEL Chatbot with Memory\")\n",
        "    \n",
        "    # Create a chatbot UI that displays messages in a conversational format\n",
        "    chatbot_ui = gr.Chatbot(type=\"messages\")\n",
        "    \n",
        "    # Create a row for the input textbox and buttons\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(placeholder=\"Type a message...\")  # Input box for user messages\n",
        "        submit_btn = gr.Button(\"Send\")                    # Button to send messages\n",
        "        clear_btn = gr.Button(\"Clear\")                    # Button to clear the chat history\n",
        "\n",
        "    # Link the \"Send\" button and the textbox to the chatbot function\n",
        "    submit_btn.click(fn=chatbot, inputs=txt, outputs=chatbot_ui)\n",
        "    txt.submit(fn=chatbot, inputs=txt, outputs=chatbot_ui)\n",
        "    \n",
        "    # Link the \"Clear\" button to the clear_history function\n",
        "    clear_btn.click(fn=clear_history, inputs=None, outputs=chatbot_ui)\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3PrJhxp4nzMK",
      "metadata": {
        "id": "3PrJhxp4nzMK"
      },
      "source": [
        "The app is working fine, but for better responses I would suggest changing the model to a better one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "jSUEbpK5lSYF",
      "metadata": {
        "id": "jSUEbpK5lSYF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
